# Contributing Guide

## Development Workflow

This document explains how to work on this project, the difference between notebooks and scripts, and how the codebase is organized.

---

## üèóÔ∏è Project Structure Philosophy

This project follows a **dual-track approach**:

1. **Notebooks** (`notebooks/`) - For exploration, experimentation, and documentation
2. **Scripts** (`src/`) - For production-ready, modular, testable code

---

## üìä Notebooks vs Scripts: When to Use Each

### Jupyter Notebooks (.ipynb)

**Purpose:** Exploration, analysis, experimentation, and communication

**Use notebooks for:**
- ‚úÖ Exploratory Data Analysis (EDA)
- ‚úÖ Feature engineering experimentation
- ‚úÖ Model prototyping and comparison
- ‚úÖ Generating visualizations for reports
- ‚úÖ Documenting your thought process
- ‚úÖ Showing results to stakeholders

**Characteristics:**
- Interactive execution (cell by cell)
- Inline visualizations
- Mix of code, markdown, and outputs
- Great for storytelling
- Saved outputs for documentation

**Limitations:**
- Not easily executable as a pipeline
- Harder to version control (JSON format)
- Not modular or reusable
- Not suitable for production deployment

---

### Python Scripts (.py)

**Purpose:** Production code, automation, modularity, and reusability

**Use scripts for:**
- ‚úÖ Production pipelines
- ‚úÖ Modular, reusable functions
- ‚úÖ Code that needs to be tested
- ‚úÖ CLI tools
- ‚úÖ Code that will be imported by other modules
- ‚úÖ Deployment-ready components

**Characteristics:**
- Executable from command line
- Easy to version control (plain text)
- Modular and testable
- Can be parameterized via CLI arguments
- Suitable for CI/CD pipelines

---

## üîÑ Development Workflow

### Phase 1: Exploration (Current)

**Location:** `notebooks/`

**Process:**
1. Start with a notebook for exploration
2. Experiment with different approaches
3. Visualize results interactively
4. Document findings with markdown
5. Keep the notebook as documentation

**Current notebooks:**
- `01_eda_exploration.ipynb` - Data exploration and insights
- `02_feature_engineering.ipynb` - Feature creation and selection
- `03_model_training.ipynb` - Model development and comparison (upcoming)
- `04_model_evaluation.ipynb` - Performance analysis and interpretation (upcoming)

---

### Phase 2: Production (Future - Optional)

**Location:** `src/`

**Process:**
1. Extract working logic from notebooks
2. Refactor into modular functions/classes
3. Add docstrings and type hints
4. Write unit tests
5. Create CLI scripts
6. Integrate into pipeline

**Conversion example:**

**From notebook:**
```python
# In 02_feature_engineering.ipynb
df['credit_util_rate'] = df['revol_util'] / 100
df['payment_to_income'] = (df['installment'] * 12) / (df['annual_inc'] + 1)
```

**To script:**
```python
# src/data/feature_engineer.py

class FeatureEngineer:
    """Create domain-driven features for credit risk modeling."""
    
    def create_credit_utilization(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Create credit utilization rate feature.
        
        Args:
            df: Input DataFrame with revol_util column
            
        Returns:
            DataFrame with credit_util_rate feature
        """
        df = df.copy()
        df['credit_util_rate'] = df['revol_util'] / 100
        return df
    
    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """Apply all feature engineering steps."""
        df = self.create_credit_utilization(df)
        df = self.create_payment_to_income(df)
        # ... more transformations
        return df
```

---

## üìÅ Directory Structure Explained

### `notebooks/`
**Purpose:** Interactive analysis and experimentation

**Contents:**
- EDA notebooks with visualizations
- Feature engineering experiments
- Model training comparisons
- Evaluation and interpretation

**Guidelines:**
- One notebook per major task
- Name with numerical prefix (01_, 02_)
- Include markdown cells explaining reasoning
- Clear outputs (don't commit with huge outputs)
- Executable from top to bottom (Run All should work)

---

### `src/`
**Purpose:** Production-ready, modular code

**Structure:**
```
src/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py      # Load raw data
‚îÇ   ‚îú‚îÄ‚îÄ data_cleaner.py     # Clean and preprocess
‚îÇ   ‚îî‚îÄ‚îÄ feature_engineer.py # Create features
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ train.py            # Model training logic
‚îÇ   ‚îú‚îÄ‚îÄ predict.py          # Prediction logic
‚îÇ   ‚îî‚îÄ‚îÄ evaluate.py         # Evaluation metrics
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ config.py           # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ logger.py           # Logging setup
‚îÇ   ‚îî‚îÄ‚îÄ visualization.py    # Plotting utilities
‚îî‚îÄ‚îÄ pipeline.py             # End-to-end pipeline orchestration
```

**Guidelines:**
- One module per logical component
- Clear function/class names
- Comprehensive docstrings
- Type hints for function signatures
- Importable by other modules

---

### `tests/`
**Purpose:** Unit tests for src/ modules

**Structure:**
```
tests/
‚îú‚îÄ‚îÄ test_data_loader.py
‚îú‚îÄ‚îÄ test_feature_engineer.py
‚îî‚îÄ‚îÄ test_models.py
```

**Guidelines:**
- Mirror src/ structure
- Test critical functions
- Use pytest fixtures for setup
- Aim for >70% code coverage

---

### `scripts/`
**Purpose:** CLI scripts for automation

**Contents:**
- `download_data.sh` - Data acquisition
- `train_model.py` - Model training from CLI
- `evaluate_model.py` - Model evaluation from CLI

**Usage:**
```bash
python scripts/train_model.py --config config/config.yaml
```

---

### `config/`
**Purpose:** Configuration files

**Contents:**
- `config.yaml` - Main project configuration
- `feature_config.yaml` - Feature engineering parameters

**Benefits:**
- Separate config from code
- Easy to change parameters
- Version controlled
- Environment-specific configs

---

### `data/`
**Purpose:** Data storage (NOT tracked in Git)

**Structure:**
```
data/
‚îú‚îÄ‚îÄ raw/            # Original, immutable data
‚îú‚îÄ‚îÄ processed/      # Cleaned, feature-engineered data
‚îî‚îÄ‚îÄ README.md       # Data documentation
```

**Important:**
- Large data files are in .gitignore
- Only metadata/small samples in Git
- Download instructions in data/README.md

---

## üß™ Testing

### Running Tests

```bash
# All tests
pytest tests/ -v

# With coverage
pytest tests/ --cov=src --cov-report=html

# Specific test file
pytest tests/test_feature_engineer.py -v
```

### Writing Tests

```python
# tests/test_feature_engineer.py

import pytest
import pandas as pd
from src.data.feature_engineer import create_credit_utilization

def test_create_credit_utilization():
    """Test credit utilization calculation."""
    # Arrange
    df = pd.DataFrame({'revol_util': [50.0, 75.0, 100.0]})
    
    # Act
    result = create_credit_utilization(df)
    
    # Assert
    assert 'credit_util_rate' in result.columns
    assert result['credit_util_rate'].iloc[0] == 0.5
```

---

## üé® Code Style

### Python Style Guide

This project follows **PEP 8** with these tools:

- **black** - Code formatting (line length: 100)
- **flake8** - Linting
- **isort** - Import sorting
- **mypy** - Type checking (optional)

### Formatting Code

```bash
# Format all code
black src/ tests/

# Sort imports
isort src/ tests/

# Check linting
flake8 src/ tests/
```

### Pre-commit Checks

Before committing:
```bash
# Run all checks
black --check src/ tests/
flake8 src/ tests/
pytest tests/
```

---

## üìù Docstring Format

Use **Google Style** docstrings:

```python
def calculate_default_probability(
    loan_amnt: float,
    int_rate: float,
    dti: float
) -> float:
    """
    Calculate probability of loan default.
    
    Uses a trained model to predict default probability based on
    loan characteristics.
    
    Args:
        loan_amnt: Loan amount in dollars
        int_rate: Interest rate as percentage (e.g., 12.5 for 12.5%)
        dti: Debt-to-income ratio as percentage
    
    Returns:
        Probability of default between 0 and 1
    
    Raises:
        ValueError: If inputs are negative or out of valid range
    
    Example:
        >>> calculate_default_probability(10000, 12.5, 25.0)
        0.23
    """
    # Implementation here
    pass
```

---

## üîÑ Git Workflow

### Branch Strategy

For this project (solo development):
- Work directly on `main` branch
- Create feature branches for major changes (optional)

For team projects:
```
main              # Production-ready code
‚îú‚îÄ‚îÄ develop       # Active development
    ‚îú‚îÄ‚îÄ feature/eda
    ‚îú‚îÄ‚îÄ feature/modeling
    ‚îî‚îÄ‚îÄ feature/deployment
```

### Commit Messages

Use descriptive commit messages:

**Good:**
```
‚úì Add feature engineering notebook with credit risk indicators
‚úì Implement XGBoost model with hyperparameter tuning
‚úì Fix data leakage by removing payment history features
‚úì Update README with model performance results
```

**Bad:**
```
‚úó update
‚úó changes
‚úó fix
‚úó asdf
```

### Commit Frequency

Commit after completing a logical unit of work:
- ‚úÖ Completed notebook section
- ‚úÖ Implemented new feature
- ‚úÖ Fixed a bug
- ‚úÖ Updated documentation

---

## üìä Working with Notebooks

### Best Practices

1. **Clear all outputs before committing:**
   ```python
   # In Jupyter: Kernel ‚Üí Restart & Clear Output
   ```

2. **Make notebooks executable top-to-bottom:**
   - Run All should work without errors
   - No manual steps required

3. **Use meaningful cell separators:**
   ```markdown
   ## Section Name
   Brief description of what this section does
   ```

4. **Keep cells focused:**
   - One logical task per cell
   - Not too long (max ~50 lines)

5. **Save key outputs:**
   ```python
   # Save important figures
   plt.savefig('../reports/figures/feature_correlation.png')
   
   # Save processed data
   df.to_csv('../data/processed/features.csv', index=False)
   ```

---

## üöÄ Running the Pipeline

### Option 1: Notebooks (Current)

Execute notebooks in order:
```bash
# 1. EDA
jupyter notebook notebooks/01_eda_exploration.ipynb

# 2. Feature Engineering
jupyter notebook notebooks/02_feature_engineering.ipynb

# 3. Model Training (upcoming)
jupyter notebook notebooks/03_model_training.ipynb
```

### Option 2: Scripts (Future)

Run as automated pipeline:
```bash
# Full pipeline
python src/pipeline.py --config config/config.yaml

# Individual steps
python scripts/train_model.py --data data/processed/train_data.csv
python scripts/evaluate_model.py --model models/xgboost_model.pkl
```

---

## üêõ Debugging

### In Notebooks

Use these patterns:
```python
# Inspect DataFrames
df.head()
df.info()
df.describe()

# Check for issues
df.isnull().sum()
df.dtypes

# Visualize distributions
df['column'].hist()
```

### In Scripts

Use logging instead of print:
```python
import logging

logger = logging.getLogger(__name__)
logger.info(f"Processing {len(df)} records")
logger.warning("Missing values detected")
logger.error("Model training failed")
```

---

## üìö Documentation

### What to Document

1. **README.md** - Project overview, setup, usage
2. **PROJECT_SUMMARY.md** - Detailed context and methodology
3. **CONTRIBUTING.md** - This file (development guide)
4. **data/README.md** - Data sources and descriptions
5. **Docstrings** - In all functions and classes
6. **Markdown cells** - In notebooks explaining reasoning

### What NOT to Include

- ‚ùå Implementation details in README (keep it high-level)
- ‚ùå Code snippets in documentation (link to source instead)
- ‚ùå Outdated information (update or remove)

---

## ‚úÖ Checklist for New Features

Before considering a feature complete:

- [ ] Code works as expected
- [ ] Docstrings added
- [ ] Tests written (if production code)
- [ ] Notebook outputs cleared (if notebook)
- [ ] Code formatted (black, isort)
- [ ] No linting errors (flake8)
- [ ] Documentation updated
- [ ] Committed with descriptive message

---

## üéì Learning Resources

### For Notebooks
- [Jupyter Best Practices](https://jupyter-notebook.readthedocs.io/en/stable/)
- [Gallery of Interesting Notebooks](https://github.com/jupyter/jupyter/wiki)

### For Production Code
- [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html)
- [The Hitchhiker's Guide to Python](https://docs.python-guide.org/)

### For ML Projects
- [Cookiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/)
- [Rules of Machine Learning](https://developers.google.com/machine-learning/guides/rules-of-ml)

---

## ü§ù Questions?

If you're working on this project and have questions:

1. Check existing documentation (this file, README, PROJECT_SUMMARY)
2. Look at existing code for patterns
3. Review commit history for context

---

**Last Updated:** February 2026  
**Maintainer:** Federico Ceballos Torres  
**Contact:** federico.ct@gmail.com
